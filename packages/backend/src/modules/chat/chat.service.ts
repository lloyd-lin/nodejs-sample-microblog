import { Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { Observable } from 'rxjs';
import OpenAI from 'openai';
import { ChatCompletionDto, ChatResponseDto, ChatMessageDto } from './dto/chat.dto';

@Injectable()
export class ChatService {
  private readonly logger = new Logger(ChatService.name);
  private openai: OpenAI;

  constructor(private configService: ConfigService) {
    this.initializeOpenAI();
  }

  private initializeOpenAI() {
    const apiKey = this.configService.get<string>('OPENAI_API_KEY');
    const baseURL = this.configService.get<string>('OPENAI_BASE_URL');

    if (!apiKey) {
      this.logger.warn('OPENAI_API_KEY not found, using mock service');
      this.openai = null;
      return;
    }

    try {
      this.openai = new OpenAI({
        apiKey,
        baseURL: baseURL || 'https://api.openai.com/v1',
      });
      this.logger.log('OpenAI client initialized successfully');
    } catch (error) {
      this.logger.error('Failed to initialize OpenAI client:', error);
      this.openai = null;
    }
  }

  /**
   * èŠå¤©å®Œæˆ - éæµå¼
   */
  async chatCompletion(chatDto: ChatCompletionDto): Promise<ChatResponseDto> {
    try {
      const { messages, model = 'gpt-3.5-turbo', temperature = 0.7, max_tokens = 1000 } = chatDto;

      // å¦‚æœæ²¡æœ‰é…ç½®OpenAIï¼Œä½¿ç”¨æ¨¡æ‹ŸæœåŠ¡
      if (!this.openai) {
        return this.mockChatCompletion(chatDto);
      }

      // è°ƒç”¨OpenAI API
      const completion = await this.openai.chat.completions.create({
        model,
        messages: messages.map(msg => ({
          role: msg.role,
          content: msg.content,
        })),
        temperature,
        max_tokens,
        stream: false,
      });

      return {
        id: completion.id,
        object: completion.object,
        created: completion.created,
        model: completion.model,
        choices: completion.choices.map(choice => ({
          index: choice.index,
          message: {
            role: choice.message.role as 'assistant',
            content: choice.message.content || '',
          },
          finish_reason: choice.finish_reason || 'stop',
        })),
        usage: {
          prompt_tokens: completion.usage?.prompt_tokens || 0,
          completion_tokens: completion.usage?.completion_tokens || 0,
          total_tokens: completion.usage?.total_tokens || 0,
        },
      };
    } catch (error) {
      this.logger.error('Chat completion error:', error);
      
      // å¦‚æœOpenAIè°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ°æ¨¡æ‹ŸæœåŠ¡
      if (error.status === 401) {
        this.logger.warn('OpenAI API key invalid, falling back to mock service');
        return this.mockChatCompletion(chatDto);
      }
      
      throw new Error(`èŠå¤©æœåŠ¡é”™è¯¯: ${error.message}`);
    }
  }

  /**
   * èŠå¤©å®Œæˆ - æµå¼è¿”å›
   */
  chatCompletionStream(chatDto: ChatCompletionDto): Observable<string> {
    const { messages, model = 'gpt-3.5-turbo', temperature = 0.7, max_tokens = 1000 } = chatDto;

    return new Observable(observer => {
      if (!this.openai) {
        // ä½¿ç”¨æ¨¡æ‹Ÿæµå¼æœåŠ¡
        this.mockStreamResponse(chatDto).then(async (stream) => {
          for await (const chunk of stream) {
            observer.next(chunk);
          }
          observer.next('data: [DONE]\n\n');
          observer.complete();
        }).catch(error => {
          observer.error(error);
        });
        return;
      }

      // è°ƒç”¨OpenAIæµå¼API
      this.openai.chat.completions.create({
        model,
        messages: messages.map(msg => ({
          role: msg.role,
          content: msg.content,
        })),
        temperature,
        max_tokens,
        stream: true,
      }).then(async (stream) => {
        try {
          for await (const chunk of stream) {
            const sseData = this.formatOpenAIChunk(chunk);
            if (sseData) {
              observer.next(sseData);
            }
          }
          observer.next('data: [DONE]\n\n');
          observer.complete();
        } catch (error) {
          this.logger.error('Stream processing error:', error);
          observer.error(error);
        }
      }).catch(error => {
        this.logger.error('Stream creation error:', error);
        // å›é€€åˆ°æ¨¡æ‹Ÿæµå¼æœåŠ¡
        this.mockStreamResponse(chatDto).then(async (stream) => {
          for await (const chunk of stream) {
            observer.next(chunk);
          }
          observer.next('data: [DONE]\n\n');
          observer.complete();
        }).catch(mockError => {
          observer.error(mockError);
        });
      });
    });
  }

  /**
   * æ ¼å¼åŒ–OpenAIæµå¼å“åº”å—
   */
  private formatOpenAIChunk(chunk: any): string {
    try {
      const sseChunk = {
        id: chunk.id,
        object: chunk.object,
        created: chunk.created,
        model: chunk.model,
        choices: chunk.choices.map(choice => ({
          index: choice.index,
          delta: choice.delta,
          finish_reason: choice.finish_reason,
        })),
      };

      return `data: ${JSON.stringify(sseChunk)}\n\n`;
    } catch (error) {
      this.logger.error('Error formatting chunk:', error);
      return '';
    }
  }

  /**
   * æ¨¡æ‹ŸèŠå¤©å®Œæˆ
   */
  private async mockChatCompletion(chatDto: ChatCompletionDto): Promise<ChatResponseDto> {
    const { messages, model = 'gpt-3.5-turbo' } = chatDto;
    const userMessage = messages[messages.length - 1]?.content || '';
    
    let responseContent = '';
    
    if (userMessage.includes('å¾®åš') || userMessage.includes('åº”ç”¨') || userMessage.includes('åŠŸèƒ½')) {
      responseContent = `ä½ å¥½ï¼æˆ‘æ˜¯å¾®åšåº”ç”¨çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¿™ä¸ªå¾®åšåº”ç”¨åŒ…å«ä»¥ä¸‹ä¸»è¦åŠŸèƒ½ï¼š

ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**
â€¢ ç”¨æˆ·æ³¨å†Œä¸ç™»å½•ç®¡ç†
â€¢ å‘å¸ƒæ–‡å­—ã€å›¾ç‰‡åŠ¨æ€
â€¢ ç‚¹èµã€è¯„è®ºã€è½¬å‘äº’åŠ¨
â€¢ å…³æ³¨/ç²‰ä¸ç¤¾äº¤ç½‘ç»œ

ğŸ“± **ç‰¹è‰²åŠŸèƒ½**  
â€¢ è¯é¢˜æ ‡ç­¾ä¸çƒ­é—¨æ¨è
â€¢ å®æ—¶æ¶ˆæ¯é€šçŸ¥
â€¢ ä¸ªäººä¸»é¡µä¸èµ„æ–™ç®¡ç†
â€¢ å†…å®¹æœç´¢ä¸å‘ç°

ğŸ¤– **AIåŠ©æ‰‹**
â€¢ æ™ºèƒ½èŠå¤©å¯¹è¯
â€¢ å†…å®¹åˆ›ä½œå»ºè®®
â€¢ ä½¿ç”¨æŒ‡å¯¼ä¸å¸®åŠ©

æœ‰ä»€ä¹ˆå…·ä½“é—®é¢˜æˆ‘å¯ä»¥å¸®ä½ è§£ç­”å—ï¼Ÿ`;
    } else if (userMessage.includes('ä½ å¥½') || userMessage.includes('hello')) {
      responseContent = 'ä½ å¥½ï¼æˆ‘æ˜¯å¾®åšåº”ç”¨çš„AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ï¼æˆ‘å¯ä»¥å¸®ä½ äº†è§£åº”ç”¨åŠŸèƒ½ã€å›ç­”ä½¿ç”¨é—®é¢˜ã€æä¾›åˆ›ä½œå»ºè®®ç­‰ã€‚è¯·å‘Šè¯‰æˆ‘ä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©å§ï¼ğŸ˜Š';
    } else {
      responseContent = `æˆ‘ç†è§£ä½ çš„é—®é¢˜ï¼š"${userMessage}"ã€‚ä½œä¸ºå¾®åšåº”ç”¨çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºä½ æä¾›å¸®åŠ©ã€‚

ä½ å¯ä»¥å‘æˆ‘è¯¢é—®ï¼š
â€¢ å¾®åšåº”ç”¨çš„åŠŸèƒ½ä»‹ç»
â€¢ ä½¿ç”¨æ–¹æ³•å’Œæ“ä½œæŒ‡å—  
â€¢ å†…å®¹åˆ›ä½œå»ºè®®
â€¢ æŠ€æœ¯ç›¸å…³é—®é¢˜

è¯·å‘Šè¯‰æˆ‘æ›´å…·ä½“çš„éœ€æ±‚ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æä¾›æ›´å‡†ç¡®çš„è§£ç­”ï¼`;
    }

    return {
      id: this.generateId(),
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model,
      choices: [
        {
          index: 0,
          message: {
            role: 'assistant',
            content: responseContent,
          },
          finish_reason: 'stop',
        },
      ],
      usage: {
        prompt_tokens: this.countTokens(messages.map(m => m.content).join(' ')),
        completion_tokens: this.countTokens(responseContent),
        total_tokens: this.countTokens(messages.map(m => m.content).join(' ') + responseContent),
      },
    };
  }

  /**
   * æ¨¡æ‹Ÿæµå¼å“åº”
   */
  private async mockStreamResponse(chatDto: ChatCompletionDto): Promise<AsyncIterable<string>> {
    const fullResponse = await this.mockChatCompletion(chatDto);
    const content = fullResponse.choices[0].message.content;
    const chunks = content.split('');
    
    return {
      async *[Symbol.asyncIterator]() {
        for (let i = 0; i < chunks.length; i++) {
          const sseChunk = {
            id: fullResponse.id,
            object: 'chat.completion.chunk',
            created: fullResponse.created,
            model: fullResponse.model,
            choices: [
              {
                index: 0,
                delta: {
                  content: chunks[i],
                },
                finish_reason: i === chunks.length - 1 ? 'stop' : null,
              },
            ],
          };

          yield `data: ${JSON.stringify(sseChunk)}\n\n`;
          
          // æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ
          await new Promise(resolve => setTimeout(resolve, 30));
        }
      },
    };
  }

  /**
   * è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
   */
  async getModels() {
    if (this.openai) {
      try {
        const models = await this.openai.models.list();
        return models.data.filter(model => 
          model.id.includes('gpt') || 
          model.id.includes('text-davinci') ||
          model.id.includes('claude')
        );
      } catch (error) {
        this.logger.error('Failed to fetch models from OpenAI:', error);
      }
    }

    // è¿”å›é»˜è®¤æ¨¡å‹åˆ—è¡¨
    return [
      {
        id: 'gpt-3.5-turbo',
        object: 'model',
        created: 1686935002,
        owned_by: 'openai',
      },
      {
        id: 'gpt-4',
        object: 'model',
        created: 1687882411,
        owned_by: 'openai',
      },
      {
        id: 'gpt-4-turbo',
        object: 'model',
        created: 1712361441,
        owned_by: 'openai',
      },
      {
        id: 'microblog-assistant',
        object: 'model',
        created: Date.now(),
        owned_by: 'microblog',
      },
    ];
  }

  /**
   * å¥åº·æ£€æŸ¥
   */
  async healthCheck() {
    const status = {
      openai_connected: false,
      api_key_configured: !!this.configService.get('OPENAI_API_KEY'),
      service_status: 'degraded',
    };

    if (this.openai) {
      try {
        await this.openai.models.list();
        status.openai_connected = true;
        status.service_status = 'healthy';
      } catch (error) {
        this.logger.warn('OpenAI health check failed:', error.message);
      }
    }

    if (!status.openai_connected && status.api_key_configured) {
      status.service_status = 'mock';
    }

    return status;
  }

  /**
   * ç”Ÿæˆå”¯ä¸€ID
   */
  private generateId(): string {
    return `chatcmpl-${Date.now()}-${Math.random().toString(36).substring(2, 15)}`;
  }

  /**
   * ç®€å•çš„tokenè®¡æ•°
   */
  private countTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }
} 